{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7ef5df1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification\n",
    "##### CSAA2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15751b50",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Motivation\n",
    "\n",
    "* Given some annotated data, e.g. images, patient data (disease vs hgealthy)\n",
    "\n",
    "* Can we determine the class of a new image? Canwe determine if the patient has a disease or not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551895fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generalising the problem\n",
    "\n",
    "* Set of N objects with attributes $\\textbf{x}_{n}$\n",
    "\n",
    "* Each object has a target label $t_n$\n",
    "\n",
    "* Binary classification: $t_n$ = {0,1}\n",
    "\n",
    "* Multi-class classification: $t_n$ = {0,1,2, …k}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3f18fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Probabilistic v non-probabilistic classifiers\n",
    "\n",
    "* Classifier trained on $\\textbf{x}_1$,….,$\\textbf{x}_n$ and $t_1$,...,$t_n$ and then used to classify $\\textbf{x}_{new}$\n",
    "\n",
    "\n",
    "* Probabilistic classifier: probability of a class membership\n",
    "\n",
    "\n",
    "* Non-probabilistic: hard assignment \n",
    "    * $t_{new}$ = 0 or $t_{new}$ = 1\n",
    "    \n",
    "    \n",
    "* Probabilities give us more information: probability of 0.57 more informative than $t_{new}$ = 1\n",
    "\n",
    "\n",
    "* Important when misclassification comes at a high cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3b24c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### KNN\n",
    "\n",
    "* Non-probabilistic \n",
    "* Works for binary and multi-class\n",
    "* Steps: \n",
    "    * Choose K (num of nearest neighbours)\n",
    "    * For a test point $x_{new}$: \n",
    "    * Find the K closest points from the training set\n",
    "    * Find majority class of those K neighbours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1527123a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### KNN\n",
    "\n",
    "<img src=\"imgs/knn1.png\" width=350 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d863cde8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### KNN\n",
    "\n",
    "<img src=\"imgs/knn2.png\" width=350 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4658be87",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### KNN \n",
    "\n",
    "<img src=\"imgs/knn3.png\" width=350 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56920b63",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### KNN\n",
    "\n",
    "\n",
    "<img src=\"imgs/knn4.png\" width=350 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051ec93e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### KNN\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"imgs/knn5.png\" width=350 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49cc20f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### KNN - Binary Example\n",
    "\n",
    "\n",
    "Can we draw a decision boundary?\n",
    "\n",
    "<img src=\"imgs/binary_knn1.png\" width=350 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5deb54e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### KNN - Binary Example\n",
    "\n",
    "\n",
    "\n",
    "Too complex? Overfitting?\n",
    "\n",
    "<img src=\"imgs/binary_knn2.png\" width=350 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e792c1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### KNN - Binary Example\n",
    "\n",
    "\n",
    "Random assignments if ties\n",
    "\n",
    "<img src=\"imgs/binary_knn3.png\" width=350 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db44feed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### KNN - Binary Example\n",
    "\n",
    "\n",
    "A much smoother decision boundary\n",
    "<img src=\"imgs/binary_knn4.png\" width=350 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dfddcf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### KNN - Binary Example\n",
    "\n",
    "<img src=\"imgs/binary_knn5.png\" width=350 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff81e03",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problems with KNN\n",
    "\n",
    "* Class imbalance\n",
    "    * If we have many training objects for class 1 and 5 for class 2\n",
    "    \n",
    "    * If we take K>=11, class 1 will always win\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "* Choice of K\n",
    "    * Depends on the data\n",
    "    * Cross validation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6bc84c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross Validation\n",
    "\n",
    "* Helps with model selection in practice\n",
    "* How do we choose data for cross validation? Often you can just pick randomly. \n",
    "\n",
    "<img src=\"imgs/cross_val.png\" width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d128b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### KNN\n",
    "\n",
    "**Pros** :\n",
    "    \n",
    "* Easy to implement\n",
    "* No training time\n",
    "* 1 parameter to tune\n",
    "\n",
    "\n",
    "**Cons**: \n",
    "* Hard assignments\n",
    "* Requires ability to compute distances\n",
    "* High storage requirements\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe59d495",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimising\n",
    "\n",
    "* In ML, we often need to find the parameters that optimise something\n",
    "    * Minimise the loss?\n",
    "    * Maximise the likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd82864",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Support Vector Machines (SVM)\n",
    "\n",
    "* **Goal**: Find the decision boundary that maximises the margin\n",
    "\n",
    "\n",
    "* Originally binary classifiers but multiclass extensions exist\n",
    "\n",
    "\n",
    "* Excellent empirical performance and hard to beat in many applications\n",
    "\n",
    "\n",
    "* Particularly useful when number of attributes >> number of training objects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e437a2b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SVM\n",
    "\n",
    "* SVMs are linear classifiers: draw a straight line to separate 2 classes\n",
    "* Maximises the **margin**: Perpendicular distance from the line from the closest points on each side \n",
    "\n",
    "\n",
    "<img src=\"imgs/svm1.png\" width=350 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfbe4cf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SVM: Support Vectors\n",
    "\n",
    "* Define the decision boundary \n",
    "* If we discard all other data but leave the support vectors, will have the same decision boundary\n",
    "* Sparse solution\n",
    "\n",
    "\n",
    "<img src=\"imgs/support_vectors.png\" width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc61430",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SVM: hard and soft margins \n",
    "\n",
    "* Optimisation is constrained to ensure all points on the correct side of the line BUT\n",
    "    * This is not always possible\n",
    "    \n",
    "    \n",
    "* Introduce a new parameter that allows for a point to go on the wrong side of the line \n",
    "\n",
    "\n",
    "* This new parameter, C, represents an upper bound on how much influence a point has\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffaa6b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SVM: hard vs soft margins\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"imgs/hard_soft_margin.png\" width=500 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b64d8d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SVM: nonlinear classification\n",
    "\n",
    "* SVMs can find linear decision boundaries \n",
    "\n",
    "\n",
    "* What if something nonlinear is required?\n",
    "\n",
    "<img src=\"imgs/non_linear.png\" width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47bbf4d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Kernels\n",
    "\n",
    "* When optimising and predicting in the default linear case, we use dot products \n",
    "\n",
    "\n",
    "* Idea: 1. project the date into a higher space\n",
    "    \n",
    "    \n",
    "* And 2. Use dot product in that space\n",
    "\n",
    "\n",
    "* Maybe in the projected space we can separate the data with a straight line\n",
    "\n",
    "\n",
    "* Popular kernels: linear, polynomial, RBF,.. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137ed71a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Kernels\n",
    "\n",
    "* We are still finding linear boundaries \n",
    "\n",
    "\n",
    "* However, those linear boundaries are in some other space\n",
    "\n",
    "\n",
    "* Other ML algorithms can also be kernalised\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e0ef34",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes\n",
    "\n",
    "* Fit a distribution to each class\n",
    "\n",
    "\n",
    "* For each test point, we evaluate the likelihood aka how similar it is to a class\n",
    "\n",
    "\n",
    "* Bayes rule is used to compute probability\n",
    "\n",
    "\n",
    "* The features we’ve measured are independent (naive)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d69839",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes\n",
    "\n",
    "<img src=\"imgs/NB1.png\" width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4ca1b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"imgs/NB2.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74cbe3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes\n",
    "\n",
    "**Pros**\n",
    "* Probabilistic\n",
    "* Good performance\n",
    "* Low computational requirements: e.g only need to store mean and variance of Gaussians \n",
    "\n",
    "\n",
    "**Cons**\n",
    "* Fitting distributions is not that easy: you need enough data\n",
    "* What if we can’t fit a nice distribution?\n",
    "* What distribution is suitable for the data we have?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4850b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Machine Learning\n",
    "\n",
    "* Distribution over model parameters (**prior**)\n",
    "\n",
    "\n",
    "* We are after a **posterior**: what we get from updating our prior belief in the light of new evidence\n",
    "\n",
    "\n",
    "* We use the posterior to make predictions\n",
    "\n",
    "\n",
    "* Sometimes, we get an exact posterior but if we can’t some advanced techniques to allow us approximate that distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90e01a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random Forest\n",
    "\n",
    "\n",
    "* Ensemble method: “wisdom of the crowds”\n",
    "    \n",
    "    \n",
    "* Sklearn implementation includes probability: predicted class is the one with highest mean probability estimate across the trees\n",
    "    \n",
    "    \n",
    "* Using the power of different trees \n",
    "\n",
    "\n",
    "<img src=\"imgs/randomforest.png\" width=300 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528a3b0a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random Forest\n",
    "\n",
    "* Let us have N samples for training \n",
    "\n",
    "\n",
    "* Each individual tree samples N objects with replacement\n",
    "\n",
    "\n",
    "* Feature randomness: each tree picks a random subset of features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7fbb23",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluating Performance \n",
    "\n",
    "* Different classifiers (we’ve covered 4 but further options as well)\n",
    "\n",
    "\n",
    "* Which one to choose? \n",
    "\n",
    "\n",
    "* How to tune parameters?\n",
    "\n",
    "\n",
    "* Examples: \n",
    "    * 0/1 loss\n",
    "    * ROC analysis\n",
    "    * Confusion matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0588b40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 0/1 loss (accuracy)\n",
    "\n",
    "* Loss is either 0 or 1 depending if prediction is correct or incorrect\n",
    "\n",
    "\n",
    "* Average over the N objects in our test data \n",
    "\n",
    "\n",
    "* What about class imbalance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49f4bae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sensitivity and Specificity\n",
    "\n",
    "* **TP** (true positive): the number of positive class objects assigned to the positive class\n",
    "* **TN** (true negative): the number of negative class objects assigned as negative\n",
    "* **FP** (false positive): the number of negative class objects classified as positive\n",
    "* **FN** (false negative): the number of positive class objects classified as negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3aa6bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sensitivity\n",
    "\n",
    "$S_e$ = $\\frac{TP}{TP+FN}$\n",
    "\n",
    "* Proportion of positive class objects we classify as positive\n",
    "\n",
    "\n",
    "* Higher values → better\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96055a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Specificity\n",
    "\n",
    "$S_p$ = $\\frac{TN}{TN+FP}$\n",
    "\n",
    "* Proportion of negative class we classify as negative\n",
    "\n",
    "\n",
    "* Higher values → better\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f700912",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimising sensitivity and specificity\n",
    "\n",
    "* Both as high as possible \n",
    "\n",
    "* Often increasing one will decrease the other\n",
    "\n",
    "* Depends on the application\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255438d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ROC \n",
    "\n",
    "* Each point is a threshold value\n",
    "\n",
    "\n",
    "* Closer to the top left corner → the better\n",
    "\n",
    "<img src=\"imgs/roc1.png\" width=350 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede6c879",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ROC \n",
    "\n",
    "<img src=\"imgs/roc2.png\" width=350 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9506fbf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ROC \n",
    "\n",
    "<img src=\"imgs/roc3.png\" width=350 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a699b54c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AUC\n",
    "\n",
    "* Quantify the performance by calculating area under the  ROC curve\n",
    "\n",
    "\n",
    "* Better than 0/1 accuracy\n",
    "\n",
    "\n",
    "* Aiming for a higher value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e67bee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Confusion Matrices\n",
    "\n",
    "* Uses TP/FP/FN/TN\n",
    "\n",
    "* Useful for multi-class classification\n",
    "\n",
    "* What classes are we likely to missclassify?\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"imgs/confusion.png\" width=350 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a96afb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Performance evaluation (Summary)\n",
    "\n",
    "**Accuracy**\n",
    "* Easy to compute \n",
    "* Sensitive to class imbalance\n",
    "\n",
    "\n",
    "**ROC/AUC** \n",
    "* Good summary metric of performance over different thresholds\n",
    "* Good for class imbalance\n",
    "* Traditional case is for binary classification\n",
    "\n",
    "\n",
    "**Confusion matrix**\n",
    "* Good for understanding multiclass results\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
